def galaxy_zoo(training_config, perc=1.0):
    from torch.utils.data.sampler import SubsetRandomSampler
    batch_size=training_config['batch_size']
    test_batch_size=training_config['test_batch_size']
    size=training_config['img_size']
    crop_size=training_config['crop_size']
    transform_train = transforms.Compose([
            transforms.Grayscale(num_output_channels=1),
            transforms.CenterCrop((crop_size,crop_size)),
            transforms.Resize(size),
            transforms.ToTensor(),
            transforms.Normalize((0.5,), (0.5,)),
        ])

    # transform_test = transforms.Compose([
    #     transforms.Grayscale(num_output_channels=1),
    #     # transforms.CenterCrop((64,64)),
    #     transforms.ToTensor(),
    #     transforms.Normalize((0.5,), (0.5,)),
    # ])

    # gz_root = '/home/cs19mtech11019/cs19mtech11024/imageFolder'
    # gz_root = '/content/drive/My Drive/imageFolder'
    gz_root = '/mnt/f/IITH/research/physics/galaxy_zoo/GalaxyClassification/imageFolder_small'

    gz_dataset = datasets.ImageFolder(root=gz_root
            # ,train=True, download=True
            , transform=transform_train
        )

    # total_images = len(gz_dataset)

    # train_dataset, test_dataset = random_split(gz_dataset,[
    #     int(0.9*total_images),
    #     int(0.1*total_images)
    # ])

    split = .9
    shuffle_dataset = True
    random_seed= 42

    # Creating data indices for training and validation splits:
    dataset_size = len(gz_dataset)
    indices = list(range(dataset_size))
    split = int(np.floor(split * dataset_size))
    if shuffle_dataset :
        np.random.seed(random_seed)
        np.random.shuffle(indices)
    train_indices, test_indices = indices[:split], indices[split:]

    # Creating PT data samplers and loaders:
    train_sampler = SubsetRandomSampler(train_indices)
    test_sampler = SubsetRandomSampler(test_indices)



    train_loader = DataLoader(gz_dataset
        ,batch_size=batch_size,
        shuffle=False, num_workers=1, drop_last=True
        ,sampler=train_sampler
    )

    # train_eval_loader = DataLoader(validation_dataset
    #     ,batch_size=test_batch_size, shuffle=True, num_workers=2, drop_last=True
    # )

    test_loader = DataLoader(gz_dataset
        ,batch_size=test_batch_size,
        shuffle=False, num_workers=1, drop_last=True
        ,sampler=test_sampler
    )

    return train_loader, test_loader